#!/usr/bin/env python3
"""Check for broken links in Hugo blog posts.

Scans all Markdown posts for internal and external links, then validates them.
- Internal links: checks that the target file exists in static/ or content/
- External links: sends HEAD requests to verify HTTP status codes

Usage:
    uv run scripts/check_links.py                    # check all links
    uv run scripts/check_links.py --internal-only    # only check internal links
    uv run scripts/check_links.py --external-only    # only check external links
    uv run scripts/check_links.py --post "slug"      # check a specific post
    uv run scripts/check_links.py --timeout 10       # custom timeout in seconds
"""

import argparse
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urlparse

import requests

BLOG_ROOT = Path(__file__).resolve().parent.parent
CONTENT_DIR = BLOG_ROOT / "content" / "posts"
STATIC_DIR = BLOG_ROOT / "static"

# Regex patterns for Markdown links
INLINE_LINK = re.compile(r"\[([^\]]*)\]\(([^)]+)\)")
REFERENCE_DEF = re.compile(r"^\s*\[([^\]]+)\]:\s*(.+)$", re.MULTILINE)

# Skip these URL schemes
SKIP_SCHEMES = {"mailto", "tel", "javascript", "data"}

# User-Agent to avoid bot blocks
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) blog-link-checker/1.0"
}


def extract_links(filepath: Path) -> list[dict]:
    """Extract all links from a Markdown file."""
    text = filepath.read_text(encoding="utf-8")

    # Strip front matter
    if text.startswith("---"):
        end = text.find("---", 3)
        if end != -1:
            text = text[end + 3 :]

    links = []
    seen = set()

    # Inline links: [text](url)
    for match in INLINE_LINK.finditer(text):
        url = match.group(2).strip()
        if url not in seen:
            seen.add(url)
            links.append({"url": url, "source": filepath.name})

    # Reference-style link definitions: [id]: url
    for match in REFERENCE_DEF.finditer(text):
        url = match.group(2).strip()
        if url not in seen:
            seen.add(url)
            links.append({"url": url, "source": filepath.name})

    return links


def classify_link(url: str) -> str:
    """Classify a link as 'internal', 'external', or 'skip'."""
    parsed = urlparse(url)

    if parsed.scheme in SKIP_SCHEMES:
        return "skip"
    if parsed.scheme in ("http", "https"):
        return "external"
    if url.startswith("/") or url.startswith("../"):
        return "internal"
    if not parsed.scheme and not parsed.netloc:
        return "internal"
    return "external"


def check_internal_link(url: str) -> tuple[str, int | None, str]:
    """Check if an internal link resolves to a file in static/ or content/."""
    clean_url = url.split("#")[0].split("?")[0]  # strip fragment/query

    if not clean_url or clean_url == "/":
        return url, None, "ok"

    # Check static directory
    static_path = STATIC_DIR / clean_url.lstrip("/")
    if static_path.exists():
        return url, None, "ok"

    # Check if it's a Hugo page route (e.g., /blog/2014/...)
    # These are generated by Hugo, so we check content/ for the source
    if clean_url.startswith("/blog/"):
        return url, None, "ok (route)"

    return url, None, "broken"


def check_external_link(
    url: str, timeout: float
) -> tuple[str, int | None, str]:
    """Check an external URL via HEAD request, falling back to GET."""
    try:
        resp = requests.head(
            url, headers=HEADERS, timeout=timeout, allow_redirects=True
        )
        if resp.status_code == 405:
            # HEAD not allowed, try GET
            resp = requests.get(
                url, headers=HEADERS, timeout=timeout, allow_redirects=True, stream=True
            )
        status = resp.status_code
        if status < 400:
            return url, status, "ok"
        return url, status, "broken"
    except requests.exceptions.SSLError:
        return url, None, "ssl_error"
    except requests.exceptions.ConnectionError:
        return url, None, "connection_error"
    except requests.exceptions.Timeout:
        return url, None, "timeout"
    except requests.exceptions.RequestException as e:
        return url, None, f"error: {e}"


def main():
    parser = argparse.ArgumentParser(description="Check broken links in Hugo blog posts")
    parser.add_argument("--internal-only", action="store_true", help="Only check internal links")
    parser.add_argument("--external-only", action="store_true", help="Only check external links")
    parser.add_argument("--post", default=None, help="Only check a specific post slug")
    parser.add_argument("--timeout", type=float, default=10.0, help="HTTP timeout in seconds (default: 10)")
    parser.add_argument("--workers", type=int, default=10, help="Number of concurrent workers for external checks (default: 10)")
    args = parser.parse_args()

    # Gather posts
    if args.post:
        matches = [p for p in CONTENT_DIR.glob("*.md") if args.post in p.name]
        if not matches:
            print(f"Error: No post found matching '{args.post}'")
            sys.exit(1)
        posts = matches
    else:
        posts = sorted(CONTENT_DIR.glob("*.md"))

    # Extract all links
    all_links = []
    for post_path in posts:
        all_links.extend(extract_links(post_path))

    # Classify links
    internal_links = []
    external_links = []
    skipped = 0

    for link in all_links:
        kind = classify_link(link["url"])
        if kind == "skip":
            skipped += 1
        elif kind == "internal":
            internal_links.append(link)
        else:
            external_links.append(link)

    print(f"Found {len(internal_links)} internal + {len(external_links)} external links ({skipped} skipped)\n")

    broken = []

    # Check internal links
    if not args.external_only:
        print(f"Checking {len(internal_links)} internal links...")
        for link in internal_links:
            url, status, result = check_internal_link(link["url"])
            if result == "broken":
                broken.append({**link, "status": status, "result": result, "type": "internal"})

    # Check external links
    if not args.internal_only:
        print(f"Checking {len(external_links)} external links (workers={args.workers})...")
        with ThreadPoolExecutor(max_workers=args.workers) as pool:
            futures = {}
            for link in external_links:
                future = pool.submit(check_external_link, link["url"], args.timeout)
                futures[future] = link

            done = 0
            total = len(external_links)
            for future in as_completed(futures):
                done += 1
                link = futures[future]
                url, status, result = future.result()
                if result != "ok":
                    broken.append({**link, "status": status, "result": result, "type": "external"})
                if done % 10 == 0 or done == total:
                    print(f"  [{done}/{total}] checked...")

    # Report
    print(f"\n{'=' * 60}")
    if not broken:
        print("No broken links found!")
    else:
        print(f"Found {len(broken)} broken links:\n")
        for b in sorted(broken, key=lambda x: (x["source"], x["type"])):
            status_str = f" (HTTP {b['status']})" if b["status"] else f" ({b['result']})"
            print(f"  [{b['type'].upper():8s}] {b['source']}")
            print(f"             {b['url']}{status_str}")
            print()

    print(f"{'=' * 60}")
    total_checked = (
        (len(internal_links) if not args.external_only else 0)
        + (len(external_links) if not args.internal_only else 0)
    )
    print(f"Total checked: {total_checked} | Broken: {len(broken)} | OK: {total_checked - len(broken)}")

    sys.exit(1 if broken else 0)


if __name__ == "__main__":
    main()
